HDF5 Library: Version 1.8.16
rank 0: ==== Parameters ====
rank 0: IO API=phdf5 
rank 0: Number of files=1
rank 0: Number of datasets=1
rank 0: Number of iterations=5
rank 0: Number of processes=128:128
rank 0: Number of bytes per process per dataset=1GB
rank 0: Size of dataset(s)=128GB:128GB
rank 0: File size=128GB:128GB
rank 0: Transfer buffer size=1GB:1GB
rank 0: Block size=1GB
rank 0: Block Pattern in Dataset=Contiguous
rank 0: I/O Method for MPI and HDF5=Collective
rank 0: Geometry=1D
rank 0: VFL used for HDF5 I/O=MPI-IO driver
rank 0: Data storage method in HDF5=Contiguous
rank 0: Env HDF5_PARAPREFIX=/scratch/sciteam/heber/4383648.bw
rank 0: Dumping MPI Info Object(469762048) (up to 1024 bytes per item):
object is MPI_INFO_NULL
rank 0: ==== End of Parameters ====

Number of processors = 128
Transfer Buffer Size: 1073741824 bytes, File size: 131072.00 MBs
      # of files: 1, # of datasets: 1, dataset size: 131072.00 MBs
        IO API = PHDF5 (w/MPI-IO driver)
            Write (5 iteration(s)):
                Maximum Throughput: 17379.29 MB/s
                Average Throughput: 16687.14 MB/s
                Minimum Throughput: 15394.35 MB/s
            Write Open-Close (5 iteration(s)):
                Maximum Throughput: 14590.34 MB/s
                Average Throughput: 14134.43 MB/s
                Minimum Throughput: 13061.75 MB/s
            Read (5 iteration(s)):
                Maximum Throughput: 11682.53 MB/s
                Average Throughput: 11389.04 MB/s
                Minimum Throughput: 10946.15 MB/s
            Read Open-Close (5 iteration(s)):
                Maximum Throughput: 11593.30 MB/s
                Average Throughput: 11281.02 MB/s
                Minimum Throughput: 10842.26 MB/s
Application 37294657 resources: utime ~7338s, stime ~4018s, Rss ~1068772, inblocks ~1343901345, outblocks ~1345666605
